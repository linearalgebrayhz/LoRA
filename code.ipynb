{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class LoRAlinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, merge, rank = 16, lora_alpha = 16, dropout = 0.5) -> None:\n",
    "        super(LoRAlinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.merge = merge\n",
    "        self.rank = rank\n",
    "        self.lora_alpha = lora_alpha\n",
    "        self.dropout_rate = dropout\n",
    "        \n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        if rank > 0:\n",
    "            self.lora_b = nn.Parameter(torch.zeros(out_features,rank))\n",
    "            self.lora_a = nn.Parameter(torch.zeros(rank,in_features))\n",
    "            self.scale = self.lora_alpha/self.rank # why\n",
    "            self.linear.weight.requires_grad = False\n",
    "        \n",
    "        if self.dropout_rate > 0:\n",
    "             self.dropout = nn.Dropout(self.dropout_rate)\n",
    "        else:\n",
    "            self.dropout = nn.Identity()\n",
    "            \n",
    "    def initial_weights(self):\n",
    "        nn.init.kaiming_uniform_(self.lora_a, a = math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_b)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.rank > 0 and self.merge:\n",
    "            output = F.linear(x, self.linear.weight + self.lora_a @ self.lora_b * self.scale, self.linear.bias)\n",
    "            output = self.dropout(output)\n",
    "            return output\n",
    "        else:\n",
    "            return self.dropout(self.linear(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 什么是Focal Loss\n",
    "分类问题我们通常会使用交叉熵损失函数，但是交叉熵损失函数对于类别不均衡的问题并不是很友好，因为它会对少数类别的样本给予更多的权重，而对于多数类别的样本给予较少的权重。为了解决这个问题，Lin等人提出了Focal Loss，Focal Loss是一种专门用于处理类别不均衡问题的损失函数，它通过调整损失函数的权重，使得模型更加关注难以分类的样本，从而提高模型的泛化能力。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5506, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha = None, gamma = 2.0, reduction = 'mean'):\n",
    "        \"\"\"_summary_\n",
    "        initialize FocalLoss Class\n",
    "        Args:\n",
    "            alpha (_type_, optional): class weight, float array with same size as categories. Defaults to None.\n",
    "            gamma (float, optional): reducing the contribution of \"easy\" classes to the Loss. Defaults to 2.0.\n",
    "            reduction (str, optional): return mode of Loss. Defaults to 'mean'.\n",
    "        \"\"\"\n",
    "        super(FocalLoss,self).__init__()\n",
    "        if alpha is None:\n",
    "            self.alpha = torch.tensor([1.0])\n",
    "        else:\n",
    "            self.alpha = torch.tensor(alpha)\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        \n",
    "    def forward(self, inputs, labels):\n",
    "        \"\"\"\n",
    "        compute Focal Loss\n",
    "        Args:\n",
    "            inputs (_type_): logits from model, shape (N,C) where N is the number of samples and C is the number of categories\n",
    "            targets (_type_): True class labels with shape (N,)\n",
    "        \"\"\"\n",
    "        BCE_loss = F.cross_entropy(inputs, labels, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        \n",
    "        alpha_t = self.alpha.to(inputs.device)\n",
    "        alpha_t = alpha_t.gather(0,labels.data.view(-1))\n",
    "        loss = alpha_t * (1-pt) ** self.gamma * BCE_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "criterion = FocalLoss(alpha=[0.25, 0.5, 1.0], gamma = 2.0, reduction = 'mean')\n",
    "inputs = torch.randn(10, 3, requires_grad=True)\n",
    "labels = torch.empty(10, dtype=torch.long).random_(3) # 随机生成十个类别标签，范围0-2\n",
    "\n",
    "loss = criterion(inputs, labels)\n",
    "print(loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
